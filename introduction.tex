\chapter{Introduction}\label{introduction}
Term rewriting systems are a simple yet powerful tool used all over, with $\lambda$-calculus perhaps being the best known example. Term rewriting systems can also be found in definitions of functional languages and other calculi. The rewriting systems have been around for decades and most definitely will be around for many more. They are still actively researched. 

The basic notion of term rewriting systems can be seen as a set of rules. These rules can be used to ``translate'' terms into other terms. Terms, in their place, consist of functions and variables. An example could be as follows.
\[
\begin{array}{lrcl}
r_1: & f(x) & \rightarrow & g(x) \\
r_2: & 0 & \rightarrow & 1
\end{array}
\]
Here, $f$, $g$, $0$ and $1$ are functions, and $x$ is a variable. These rules define how terms can be rewritten. If we were to have a term such as $g(f(1))$, we can use $r_1$ to rewrite the term to $g(g(1))$. We can do this because in $r_1$ the argument of $f$ is a variable. So if we set $x$ to be $1$, then we should rewrite $f(1)$ to $g(1)$. Which is exactly what we did with $g(f(1))$. While $0$ and $1$ might not seem like functions, they are. We can also write them down as $0()$ and $1()$. They are function with no arguments, so they can be seen as constants. When using constants we usually omit the parenthesis. 

One of these many branches of research focuses on termination and non-termination of term rewriting systems. Termination and non-termination are actively researched since they are properties we want to know about. We can for example translate (functional) languages into term rewriting systems to then look at the termination properties of that code. Even though we know that (non-)termination of programs is generally undecidable, we are increasing the class of programs of which we \textit{can} prove those properties further and further.

Previous research focused on unsorted term rewriting systems, where variables and functions do not have specific type definitions. In many-sorted term rewriting systems, variables and functions do have specific type definitions, specifically first-order types. 

For example, without types we could create a term such as $add(5, \text{ red})$. This can be done since we have never said what type of input the $add$ function gets. We of course know that $5$ is a number and red is a colour, but by default rewriting systems do not. By adding types, we can define that the $add$ function gets two numbers as arguments.

In previous work, many techniques for detecting (non-)termination have been created. These techniques are not made for (many-)sorted terms, and therefore will not work for them without adaptations. Certain steps in the techniques or algorithms might be illegal when working in a sorted rewriting system, since they do not account for types by design. Think for example about the $add$ function. Some technique might create a term such as $add(4, blue)$, but we do not want those anymore. 

These adaptations, even though they might only be small, can have impact on the strength of the technique. For example, a technique that focuses on creating as many rules as possible might create much less rules when they have type bounds. This could be an improvement to the technique but it might also result in a decrease of its effectiveness. With less rules to analyse the analyser runs faster but gets fewer opportunities to find non-termination in a system. 

We will take a look at a number of techniques to prove non-termination and adapt them to make them work for many-sorted rewriting systems. The adaptations will be implemented in an analyser in Java called Mara. Mara stands for MAny-sorted Rewriting Analyser. As a basis for parsing and basic functionality Cora will be used. Cora stands for COnstrained Rewriting Analyser. Cora focuses on higher order term rewriting but still can be used as a basis for Mara since it also contains parsing and typing for many-sorted terms. 

In chapter \ref{preliminaries} the necessary preliminaries will be given. Chapter \ref{analysers} gives some more information on the analysis methods that will be implemented. Then chapters \ref{matching-unification}, \ref{semi-unification} and \ref{unfolding} are about transforming the existing techniques to make them work in the many-sorted rewriting setting. Experiments are given in chapter \ref{experiments}. These experiments are mainly to compare different configurations of our tool, but we can also compare it to the existing tools in competitions. The results of these experiments are given in chapter \ref{resultsexperiments}. Then chapter \ref{relatedwork} gives some background information on existing techniques and tools. Finally we get chapter \ref{futurework} for future work within the subject and chapter \ref{conclusions} concludes the work and experiments. 